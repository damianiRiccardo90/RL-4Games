{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_14_Imagination.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C14-From_DRL_to_AGI/Chapter_14_Imagination.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from actor_critic import ActorCritic\n",
        "from multiprocessing_env import SubprocVecEnv\n",
        "from minipacman import MiniPacman\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "#7 different pixels in MiniPacman\n",
        "pixels = (\n",
        "    (0.0, 1.0, 1.0),\n",
        "    (0.0, 1.0, 0.0), \n",
        "    (0.0, 0.0, 1.0),\n",
        "    (1.0, 1.0, 1.0),\n",
        "    (1.0, 1.0, 0.0), \n",
        "    (0.0, 0.0, 0.0),\n",
        "    (1.0, 0.0, 0.0),\n",
        ")\n",
        "pixel_to_categorical = {pix:i for i, pix in enumerate(pixels)} \n",
        "num_pixels = len(pixels)\n",
        "\n",
        "#For each mode in MiniPacman there are different rewards\n",
        "mode_rewards = {\n",
        "    \"regular\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
        "    \"avoid\":   [0.1, -0.1, -5, -10, -20],\n",
        "    \"hunt\":    [0, 1, 10, -20],\n",
        "    \"ambush\":  [0, -0.1, 10, -20],\n",
        "    \"rush\":    [0, -0.1, 9.9]\n",
        "}\n",
        "reward_to_categorical = {mode: {reward:i for i, reward in enumerate(mode_rewards[mode])} for mode in mode_rewards.keys()}\n",
        "\n",
        "def pix_to_target(next_states):\n",
        "    target = []\n",
        "    for pixel in next_states.transpose(0, 2, 3, 1).reshape(-1, 3):\n",
        "        target.append(pixel_to_categorical[tuple([np.ceil(pixel[0]), np.ceil(pixel[1]), np.ceil(pixel[2])])])\n",
        "    return target\n",
        "\n",
        "def target_to_pix(imagined_states):\n",
        "    pixels = []\n",
        "    to_pixel = {value: key for key, value in pixel_to_categorical.items()}\n",
        "    for target in imagined_states:\n",
        "        pixels.append(list(to_pixel[target]))\n",
        "    return np.array(pixels)\n",
        "\n",
        "def rewards_to_target(mode, rewards):\n",
        "    target = []\n",
        "    for reward in rewards:\n",
        "        target.append(reward_to_categorical[mode][reward])\n",
        "    return target\n",
        "\n",
        "def plot(frame_idx, rewards, losses):\n",
        "    #clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('loss %s' % losses[-1])\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n",
        "    \n",
        "def displayImage(image, step, reward):\n",
        "    s = str(step) + \" \" + str(reward)\n",
        "    plt.title(s)\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_shape, n1, n2, n3):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        self.in_shape = in_shape\n",
        "        self.n1 = n1\n",
        "        self.n2 = n2\n",
        "        self.n3 = n3\n",
        "        \n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=in_shape[1:])\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_shape[0] * 2, n1, kernel_size=1, stride=2, padding=6),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n1, n1, kernel_size=10, stride=1, padding=(5, 6)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_shape[0] * 2, n2, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(n2, n2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(n1 + n2,  n3, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        x = self.pool_and_inject(inputs)\n",
        "        x = torch.cat([self.conv1(x), self.conv2(x)], 1)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.cat([x, inputs], 1)\n",
        "        return x\n",
        "    \n",
        "    def pool_and_inject(self, x):\n",
        "        pooled     = self.maxpool(x)\n",
        "        tiled      = pooled.expand((x.size(0),) + self.in_shape)\n",
        "        out        = torch.cat([tiled, x], 1)\n",
        "        return out\n",
        "\n",
        "class EnvModel(nn.Module):\n",
        "    def __init__(self, in_shape, num_pixels, num_rewards):\n",
        "        super(EnvModel, self).__init__()\n",
        "        \n",
        "        width  = in_shape[1]\n",
        "        height = in_shape[2]\n",
        "        \n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(8, 64, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.basic_block1 = BasicBlock((64, width, height), 16, 32, 64)\n",
        "        self.basic_block2 = BasicBlock((128, width, height), 16, 32, 64)\n",
        "        \n",
        "        self.image_conv = nn.Sequential(\n",
        "            nn.Conv2d(192, 256, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.image_fc = nn.Linear(256, num_pixels)\n",
        "        \n",
        "        self.reward_conv = nn.Sequential(\n",
        "            nn.Conv2d(192, 64, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.reward_fc    = nn.Linear(64 * width * height, num_rewards)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        \n",
        "        x = self.conv(inputs)\n",
        "        x = self.basic_block1(x)\n",
        "        x = self.basic_block2(x)\n",
        "        \n",
        "        image = self.image_conv(x)\n",
        "        image = image.permute(0, 2, 3, 1).contiguous().view(-1, 256)\n",
        "        image = self.image_fc(image)\n",
        "\n",
        "        reward = self.reward_conv(x)\n",
        "        reward = reward.view(batch_size, -1)\n",
        "        reward = self.reward_fc(reward)\n",
        "        \n",
        "        return image, reward\n",
        "\n",
        "\n",
        "def main():\n",
        "    mode = \"regular\"\n",
        "    num_envs = 16\n",
        "\n",
        "    def make_env():\n",
        "        def _thunk():\n",
        "            env = MiniPacman(mode, 1000)\n",
        "            return env\n",
        "\n",
        "        return _thunk\n",
        "\n",
        "    envs = [make_env() for i in range(num_envs)]\n",
        "    envs = SubprocVecEnv(envs)\n",
        "\n",
        "    state_shape = envs.observation_space.shape\n",
        "    num_actions = envs.action_space.n\n",
        "\n",
        "    env_model    = EnvModel(envs.observation_space.shape, num_pixels, len(mode_rewards[\"regular\"]))\n",
        "    actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(env_model.parameters())\n",
        "\n",
        "    actor_critic.load_state_dict(torch.load(\"actor_critic_\" + mode))\n",
        "\n",
        "    def get_action(state):\n",
        "        if state.ndim == 4:\n",
        "            state = torch.FloatTensor(np.float32(state))\n",
        "        else:\n",
        "            state = torch.FloatTensor(np.float32(state)).unsqueeze(0)\n",
        "        \n",
        "        action = actor_critic.act(autograd.Variable(state, volatile=True))\n",
        "        action = action.data.cpu().squeeze(1).numpy()\n",
        "        return action\n",
        "\n",
        "    def play_games(envs, frames):\n",
        "        states = envs.reset()\n",
        "    \n",
        "        for frame_idx in range(frames):\n",
        "            actions = get_action(states)\n",
        "            next_states, rewards, dones, _ = envs.step(actions)\n",
        "        \n",
        "            yield frame_idx, states, actions, rewards, next_states, dones\n",
        "        \n",
        "            states = next_states\n",
        "\n",
        "    reward_coef = 0.1\n",
        "    num_updates = 5000\n",
        "\n",
        "    losses = []\n",
        "    all_rewards = []\n",
        "\n",
        "    for frame_idx, states, actions, rewards, next_states, dones in tqdm(play_games(envs, num_updates), total=num_updates):\n",
        "        states      = torch.FloatTensor(states)\n",
        "        actions     = torch.LongTensor(actions)\n",
        "\n",
        "        batch_size = states.size(0)\n",
        "    \n",
        "        onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
        "        onehot_actions[range(batch_size), actions] = 1\n",
        "        inputs = autograd.Variable(torch.cat([states, onehot_actions], 1))\n",
        "    \n",
        "        #if USE_CUDA:\n",
        "        #    inputs = inputs.cuda()\n",
        "\n",
        "        imagined_state, imagined_reward = env_model(inputs)\n",
        "\n",
        "        target_state = pix_to_target(next_states)\n",
        "        target_state = autograd.Variable(torch.LongTensor(target_state))\n",
        "    \n",
        "        target_reward = rewards_to_target(mode, rewards)\n",
        "        target_reward = autograd.Variable(torch.LongTensor(target_reward))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        image_loss  = criterion(imagined_state, target_state)\n",
        "        reward_loss = criterion(imagined_reward, target_reward)\n",
        "        loss = image_loss + reward_coef * reward_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        losses.append(loss.item())\n",
        "        all_rewards.append(np.mean(rewards))\n",
        "    \n",
        "        if frame_idx % num_updates == 0:\n",
        "            plot(frame_idx, all_rewards, losses)\n",
        "\n",
        "    torch.save(env_model.state_dict(), \"env_model_\" + mode)\n",
        "\n",
        "    import time\n",
        "\n",
        "    env = MiniPacman(mode, 1000)\n",
        "    batch_size = 1\n",
        "\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    iss = []\n",
        "    ss  = []\n",
        "\n",
        "    steps = 0\n",
        "\n",
        "    while not done:\n",
        "        steps += 1\n",
        "        actions = get_action(state)\n",
        "        onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
        "        onehot_actions[range(batch_size), actions] = 1\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "    \n",
        "        inputs = autograd.Variable(torch.cat([state, onehot_actions], 1))\n",
        "        #if USE_CUDA:\n",
        "        #    inputs = inputs.cuda()\n",
        "\n",
        "        imagined_state, imagined_reward = env_model(inputs)\n",
        "        imagined_state = F.softmax(imagined_state)\n",
        "        iss.append(imagined_state)\n",
        "    \n",
        "        next_state, reward, done, _ = env.step(actions[0])\n",
        "        ss.append(state)\n",
        "        state = next_state\n",
        "    \n",
        "        imagined_image = target_to_pix(imagined_state.view(batch_size, -1, len(pixels))[0].max(1)[1].data.cpu().numpy())\n",
        "        imagined_image = imagined_image.reshape(15, 19, 3)\n",
        "        state_image = torch.FloatTensor(next_state).permute(1, 2, 0).cpu().numpy()\n",
        "    \n",
        "        #clear_output()\n",
        "        plt.figure(figsize=(10,3))\n",
        "        plt.subplot(131)\n",
        "        plt.title(\"Imagined\")\n",
        "        plt.imshow(imagined_image)\n",
        "        plt.subplot(132)\n",
        "        plt.title(\"Actual\")\n",
        "        plt.imshow(state_image)\n",
        "        plt.show()\n",
        "        time.sleep(0.3)\n",
        "    \n",
        "        if steps > 30:\n",
        "            break\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "11vK3KbIT-x_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}