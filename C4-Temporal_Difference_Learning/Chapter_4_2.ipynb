{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_4_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C4-Temporal_Difference_Learning/Chapter_4_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import system, name\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "gamma = 0.1 \n",
        "rewardSize = -1\n",
        "gridSize = 4\n",
        "alpha = 0.1 \n",
        "terminations = [[0,0], [gridSize-1, gridSize-1]]\n",
        "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]\n",
        "epochs = 10000\n",
        "\n",
        "V = np.zeros((gridSize, gridSize))\n",
        "returns = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}\n",
        "deltas = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}\n",
        "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]\n",
        "\n",
        "def generateInitialState():\n",
        "    initState = random.choice(states[1:-1])\n",
        "    return initState\n",
        "\n",
        "def generateNextAction():\n",
        "    return random.choice(actions)\n",
        "\n",
        "def takeAction(state, action):\n",
        "    if list(state) in terminations:\n",
        "        return 0, None\n",
        "    finalState = np.array(state)+np.array(action)\n",
        "    \n",
        "    if -1 in list(finalState) or gridSize in list(finalState):\n",
        "        finalState = state\n",
        "    return rewardSize, list(finalState)\n",
        "\n",
        "for it in tqdm(range(epochs)):    \n",
        "    state = generateInitialState()\n",
        "    while True:\n",
        "        action = generateNextAction()\n",
        "        reward, finalState = takeAction(state, action)\n",
        "        \n",
        "        # we reached the end\n",
        "        if finalState is None:\n",
        "            break\n",
        "        \n",
        "        # modify Value function\n",
        "        before =  V[state[0], state[1]]\n",
        "        V[state[0], state[1]] += alpha*(reward + gamma*V[finalState[0], finalState[1]] - V[state[0], state[1]])\n",
        "        deltas[state[0], state[1]].append(float(np.abs(before-V[state[0], state[1]])))\n",
        "        \n",
        "        state = finalState\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "all_series = [list(x)[:50] for x in deltas.values()]\n",
        "for series in all_series:\n",
        "    plt.plot(series)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(V)\n"
      ],
      "metadata": {
        "id": "3D_k0pTFl-wD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}