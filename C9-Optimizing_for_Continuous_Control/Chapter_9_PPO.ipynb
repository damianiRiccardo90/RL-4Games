{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_9_PPO.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C9-Optimizing_for_Continuous_Control/Chapter_9_PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "lmbda         = 0.95\n",
        "eps_clip      = 0.1\n",
        "K_epoch       = 3\n",
        "T_horizon     = 20\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(PPO, self).__init__()\n",
        "        self.data = []\n",
        "        \n",
        "        self.fc1   = nn.Linear(input_shape,256)\n",
        "        self.fc_pi = nn.Linear(256,num_actions)\n",
        "        self.fc_v  = nn.Linear(256,1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def pi(self, x, softmax_dim = 0):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc_pi(x)\n",
        "        prob = F.softmax(x, dim=softmax_dim)\n",
        "        return prob\n",
        "    \n",
        "    def v(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        v = self.fc_v(x)\n",
        "        return v\n",
        "      \n",
        "    def put_data(self, transition):\n",
        "        self.data.append(transition)\n",
        "        \n",
        "    def make_batch(self):\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
        "        for transition in self.data:\n",
        "            s, a, r, s_prime, prob_a, done = transition\n",
        "            \n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            prob_a_lst.append([prob_a])\n",
        "            done_mask = 0 if done else 1\n",
        "            done_lst.append([done_mask])\n",
        "            \n",
        "        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
        "        self.data = []\n",
        "        return s, a, r, s_prime, done_mask, prob_a\n",
        "        \n",
        "    def train_net(self):\n",
        "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
        "\n",
        "        for i in range(K_epoch):\n",
        "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
        "            delta = td_target - self.v(s)\n",
        "            delta = delta.detach().numpy()\n",
        "\n",
        "            advantage_lst = []\n",
        "            advantage = 0.0\n",
        "            for delta_t in delta[::-1]:\n",
        "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
        "                advantage_lst.append([advantage])\n",
        "            advantage_lst.reverse()\n",
        "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
        "\n",
        "            pi = self.pi(s, softmax_dim=1)\n",
        "            pi_a = pi.gather(1,a)\n",
        "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
        "\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
        "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "model = PPO(env.observation_space.shape[0], env.action_space.n)\n",
        "score = 0.0\n",
        "print_interval = 20\n",
        "iterations = 10000\n",
        "min_play_reward = 50\n",
        "\n",
        "def play_game():\n",
        "    done = False\n",
        "    state = env.reset()    \n",
        "    while(not done):        \n",
        "        prob = model.pi(torch.from_numpy(s).float())\n",
        "        m = Categorical(prob)\n",
        "        a = m.sample().item()\n",
        "        s_prime, r, done, info = env.step(a)\n",
        "        env.render()\n",
        "        state = s_prime  \n",
        "\n",
        "for iteration in range(iterations):\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        for t in range(T_horizon):\n",
        "            prob = model.pi(torch.from_numpy(s).float())            \n",
        "            m = Categorical(prob)\n",
        "            a = m.sample().item()\n",
        "            s_prime, r, done, info = env.step(a)\n",
        "\n",
        "            model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
        "            s = s_prime\n",
        "\n",
        "            score += r\n",
        "            if done:\n",
        "                if score/print_interval > min_play_reward:\n",
        "                    play_game()\n",
        "                break\n",
        "\n",
        "        model.train_net()\n",
        "\n",
        "    if iteration%print_interval==0 and iteration!=0:\n",
        "        print(\"# of episode :{}, avg score : {:.1f}\".format(iteration, score/print_interval))\n",
        "        score = 0.0\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "5gi1Nq5MK2CX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}