{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_9_PPO_LSTM.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C9-Optimizing_for_Continuous_Control/Chapter_9_PPO_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "#Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "gamma         = 0.98\n",
        "lmbda         = 0.95\n",
        "eps_clip      = 0.1\n",
        "K_epoch       = 2\n",
        "T_horizon     = 20\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(PPO, self).__init__()\n",
        "        self.data = []\n",
        "        \n",
        "        self.fc1   = nn.Linear(input_shape,64)\n",
        "        self.lstm  = nn.LSTM(64,32)\n",
        "        self.fc_pi = nn.Linear(32,num_actions)\n",
        "        self.fc_v  = nn.Linear(32,1)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def pi(self, x, hidden):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = x.view(-1, 1, 64)\n",
        "        x, lstm_hidden = self.lstm(x, hidden)\n",
        "        x = self.fc_pi(x)\n",
        "        prob = F.softmax(x, dim=2)\n",
        "        return prob, lstm_hidden\n",
        "    \n",
        "    def v(self, x, hidden):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = x.view(-1, 1, 64)\n",
        "        x, lstm_hidden = self.lstm(x, hidden)\n",
        "        v = self.fc_v(x)\n",
        "        return v\n",
        "      \n",
        "    def put_data(self, transition):\n",
        "        self.data.append(transition)\n",
        "        \n",
        "    def make_batch(self):\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, h_in_lst, h_out_lst, done_lst = [], [], [], [], [], [], [], []\n",
        "        for transition in self.data:\n",
        "            s, a, r, s_prime, prob_a, h_in, h_out, done = transition\n",
        "            \n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            prob_a_lst.append([prob_a])\n",
        "            h_in_lst.append(h_in)\n",
        "            h_out_lst.append(h_out)\n",
        "            done_mask = 0 if done else 1\n",
        "            done_lst.append([done_mask])\n",
        "            \n",
        "        s,a,r,s_prime,done_mask,prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "                                         torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "                                         torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n",
        "        self.data = []\n",
        "        return s,a,r,s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0]\n",
        "        \n",
        "    def train_net(self):\n",
        "        s,a,r,s_prime,done_mask, prob_a, (h1_in, h2_in), (h1_out, h2_out) = self.make_batch()\n",
        "        first_hidden  = (h1_in.detach(), h2_in.detach())\n",
        "        second_hidden = (h1_out.detach(), h2_out.detach())\n",
        "\n",
        "        for i in range(K_epoch):\n",
        "            v_prime = self.v(s_prime, second_hidden).squeeze(1)\n",
        "            td_target = r + gamma * v_prime * done_mask\n",
        "            v_s = self.v(s, first_hidden).squeeze(1)\n",
        "            delta = td_target - v_s\n",
        "            delta = delta.detach().numpy()\n",
        "            \n",
        "            advantage_lst = []\n",
        "            advantage = 0.0\n",
        "            for item in delta[::-1]:\n",
        "                advantage = gamma * lmbda * advantage + item[0]\n",
        "                advantage_lst.append([advantage])\n",
        "            advantage_lst.reverse()\n",
        "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
        "\n",
        "            pi, _ = self.pi(s, first_hidden)\n",
        "            pi_a = pi.squeeze(1).gather(1,a)\n",
        "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == log(exp(a)-exp(b))\n",
        "\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
        "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(v_s, td_target.detach())\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward(retain_graph=True)\n",
        "            self.optimizer.step()\n",
        "        \n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "model = PPO(env.observation_space.shape[0], env.action_space.n)\n",
        "score = 0.0\n",
        "print_interval = 20\n",
        "iterations = 10000\n",
        "    \n",
        "for iteration in range(iterations):\n",
        "    h_out = (torch.zeros([1, 1, 32], dtype=torch.float), torch.zeros([1, 1, 32], dtype=torch.float))\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "        \n",
        "    while not done:\n",
        "        for t in range(T_horizon):\n",
        "            h_in = h_out\n",
        "            prob, h_out = model.pi(torch.from_numpy(s).float(), h_in)\n",
        "            prob = prob.view(-1)\n",
        "            m = Categorical(prob)\n",
        "            a = m.sample().item()\n",
        "            s_prime, r, done, info = env.step(a)\n",
        "\n",
        "            model.put_data((s, a, r/100.0, s_prime, prob[a].item(), h_in, h_out, done))\n",
        "            s = s_prime\n",
        "\n",
        "            score += r\n",
        "            if done:\n",
        "                break\n",
        "                    \n",
        "        model.train_net()\n",
        "\n",
        "    if iteration%print_interval==0 and iteration!=0:\n",
        "        print(\"# of episode :{}, avg score : {:.1f}\".format(iteration, score/print_interval))\n",
        "        score = 0.0\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "hiCiDweFDXUW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}