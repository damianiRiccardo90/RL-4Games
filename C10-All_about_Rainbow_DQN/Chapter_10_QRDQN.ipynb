{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_10_QRDQN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C10-All_about_Rainbow_DQN/Chapter_10_QRDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd \n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from common.layers import NoisyLinear\n",
        "from common.replay_buffer import ReplayBuffer\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "env_id = \"LunarLander-v2\"\n",
        "env = gym.make(env_id)\n",
        "writer = SummaryWriter()\n",
        "\n",
        "class QRDQN(nn.Module):\n",
        "    def __init__(self, num_inputs, num_actions, num_quants):\n",
        "        super(QRDQN, self).__init__()\n",
        "        \n",
        "        self.num_inputs  = num_inputs\n",
        "        self.num_actions = num_actions\n",
        "        self.num_quants  = num_quants\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            nn.Linear(num_inputs, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.num_actions * self.num_quants)\n",
        "        )\n",
        "        \n",
        "        #self.noisy_value1 = NoisyLinear(64, 128, use_cuda=USE_CUDA)\n",
        "        #self.noisy_value2 = NoisyLinear(128, self.num_actions * self.num_quants, use_cuda=USE_CUDA)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        x = self.features(x)\n",
        "        \n",
        "        #x = self.noisy_value1(x)\n",
        "        #x = F.relu(x)\n",
        "        #x = self.noisy_value2(x)\n",
        "        x = x.view(batch_size, self.num_actions, self.num_quants)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def q_values(self, x):\n",
        "        x = self.forward(x)\n",
        "        return x.mean(2)\n",
        "    \n",
        "    def reset_noise(self):\n",
        "        self.noisy_value1.reset_noise()\n",
        "        self.noisy_value2.reset_noise() \n",
        "        \n",
        "    def act(self, state, epsilon):\n",
        "        if random.random() > epsilon:\n",
        "            state = autograd.Variable(torch.FloatTensor(np.array(state, dtype=np.float32)).unsqueeze(0), volatile=True)\n",
        "            qvalues = self.forward(state).mean(2)\n",
        "            action  = qvalues.max(1)[1]\n",
        "            action  = action.data.cpu().numpy()[0]\n",
        "        else:\n",
        "            action = random.randrange(self.num_actions)\n",
        "        return action\n",
        "\n",
        "def projection_distribution(dist, next_state, reward, done):\n",
        "    next_dist = target_model(next_state)\n",
        "    next_action = next_dist.mean(2).max(1)[1]\n",
        "    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_quant)\n",
        "    next_dist = next_dist.gather(1, next_action).squeeze(1).cpu().data\n",
        "\n",
        "    expected_quant = reward.unsqueeze(1) + 0.99 * next_dist * (1 - done.unsqueeze(1))\n",
        "    expected_quant = autograd.Variable(expected_quant)\n",
        "\n",
        "    quant_idx = torch.sort(dist, 1, descending=False)[1]\n",
        "\n",
        "    tau_hat = torch.linspace(0.0, 1.0 - 1./num_quant, num_quant) + 0.5 / num_quant\n",
        "    tau_hat = tau_hat.unsqueeze(0).repeat(batch_size, 1)\n",
        "    quant_idx = quant_idx.cpu().data\n",
        "    batch_idx = np.arange(batch_size)\n",
        "    tau = tau_hat[:, quant_idx][batch_idx, batch_idx]\n",
        "        \n",
        "    return tau, expected_quant\n",
        "\n",
        "num_quant = 51\n",
        "Vmin = -10\n",
        "Vmax = 10\n",
        "\n",
        "current_model = QRDQN(env.observation_space.shape[0], env.action_space.n, num_quant)\n",
        "target_model  = QRDQN(env.observation_space.shape[0], env.action_space.n, num_quant)\n",
        "    \n",
        "optimizer = optim.Adam(current_model.parameters())\n",
        "\n",
        "replay_buffer = ReplayBuffer(10000)\n",
        "\n",
        "def update_target(current_model, target_model):\n",
        "    target_model.load_state_dict(current_model.state_dict())\n",
        "    \n",
        "update_target(current_model, target_model)\n",
        "\n",
        "def compute_td_loss(batch_size):\n",
        "    state, action, reward, next_state, done = replay_buffer.sample(batch_size) \n",
        "\n",
        "    state      = autograd.Variable(torch.FloatTensor(np.float32(state)))\n",
        "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n",
        "    action     = autograd.Variable(torch.LongTensor(action))\n",
        "    reward     = torch.FloatTensor(reward)\n",
        "    done       = torch.FloatTensor(np.float32(done))\n",
        "\n",
        "    dist = current_model(state)\n",
        "    action = action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_quant)\n",
        "    dist = dist.gather(1, action).squeeze(1)\n",
        "    \n",
        "    tau, expected_quant = projection_distribution(dist, next_state, reward, done)\n",
        "    k = 1\n",
        "    \n",
        "    huber_loss = 0.5 * tau.abs().clamp(min=0.0, max=k).pow(2)\n",
        "    huber_loss += k * (tau.abs() -  tau.abs().clamp(min=0.0, max=k))\n",
        "    quantile_loss = (tau - (tau < 0).float()).abs() * huber_loss\n",
        "    loss = torch.tensor(quantile_loss.sum() / num_quant, requires_grad=True)\n",
        "        \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(current_model.parameters(), 0.5)\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "epsilon_start = 1.0\n",
        "epsilon_final = 0.01\n",
        "epsilon_decay = 50000\n",
        "\n",
        "epsilon_by_frame = lambda iteration: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * iteration / epsilon_decay)\n",
        "\n",
        "def plot(iteration, rewards, losses, ep_reward):    \n",
        "    print(\"Outputing Iteration \" + str(iteration))\n",
        "    writer.add_scalar('Train/Rewards', rewards[-1], iteration)\n",
        "    writer.add_scalar('Train/Losses', losses[-1], iteration) \n",
        "    writer.add_scalar('Train/Exploration', epsilon_by_frame(iteration), iteration)\n",
        "    writer.add_scalar('Train/Episode', ep_reward, iteration)\n",
        "    writer.flush()\n",
        "\n",
        "iterations = 1000000\n",
        "batch_size = 32\n",
        "gamma      = 0.98\n",
        "\n",
        "losses = []\n",
        "all_rewards = []\n",
        "episode_reward = 0\n",
        "\n",
        "state = env.reset()\n",
        "for iteration in range(1, iterations + 1):\n",
        "    action = current_model.act(state, epsilon_by_frame(iteration))\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    replay_buffer.push(state, action, reward, next_state, done)\n",
        "    \n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "    \n",
        "    if done:\n",
        "        state = env.reset()\n",
        "        all_rewards.append(episode_reward)\n",
        "        episode_reward = 0\n",
        "        \n",
        "    if len(replay_buffer) > batch_size:\n",
        "        loss = compute_td_loss(batch_size)\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "    if iteration % 200 == 0:\n",
        "        plot(iteration, all_rewards, losses, episode_reward)\n",
        "        \n",
        "    if iteration % 1000 == 0:\n",
        "        update_target(current_model, target_model)"
      ],
      "metadata": {
        "id": "YZpejzCHLS93"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}