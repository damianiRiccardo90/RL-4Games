{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_8_DDPG.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C8-Policy_Gradient_Methods/Chapter_8_DDPG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#Hyperparameters\n",
        "lr_mu        = 0.0005\n",
        "lr_q         = 0.001\n",
        "gamma        = 0.99\n",
        "batch_size   = 32\n",
        "buffer_limit = 50000\n",
        "tau          = 0.005 # for target network soft update\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "    \n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done_mask = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append([a])\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append([done_mask])\n",
        "        \n",
        "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
        "               torch.tensor(done_mask_lst)\n",
        "    \n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class MuNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MuNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(3, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc_mu = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mu = torch.tanh(self.fc_mu(x))*2 # Multipled by 2 because the action space of the Pendulum-v0 is [-2,2]\n",
        "        return mu\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNet, self).__init__()\n",
        "        \n",
        "        self.fc_s = nn.Linear(3, 64)\n",
        "        self.fc_a = nn.Linear(1,64)\n",
        "        self.fc_q = nn.Linear(128, 32)\n",
        "        self.fc_3 = nn.Linear(32,1)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        h1 = F.relu(self.fc_s(x))\n",
        "        h2 = F.relu(self.fc_a(a))\n",
        "        cat = torch.cat([h1,h2], dim=1)\n",
        "        q = F.relu(self.fc_q(cat))\n",
        "        q = self.fc_3(q)\n",
        "        return q\n",
        "\n",
        "class OrnsteinUhlenbeckNoise:\n",
        "    def __init__(self, mu):\n",
        "        self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1\n",
        "        self.mu = mu\n",
        "        self.x_prev = np.zeros_like(self.mu)\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "      \n",
        "def train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer):\n",
        "    s,a,r,s_prime,done_mask  = memory.sample(batch_size)\n",
        "    \n",
        "    target = r + gamma * q_target(s_prime, mu_target(s_prime))\n",
        "    q_loss = F.smooth_l1_loss(q(s,a), target.detach())\n",
        "    q_optimizer.zero_grad()\n",
        "    q_loss.backward()\n",
        "    q_optimizer.step()\n",
        "    \n",
        "    mu_loss = -q(s,mu(s)).mean() # That's all for the policy loss.\n",
        "    mu_optimizer.zero_grad()\n",
        "    mu_loss.backward()\n",
        "    mu_optimizer.step()\n",
        "    \n",
        "def soft_update(net, net_target):\n",
        "    for param_target, param in zip(net_target.parameters(), net.parameters()):\n",
        "        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n",
        "          \n",
        "    \n",
        "\n",
        "env = gym.make('Pendulum-v0')\n",
        "memory = ReplayBuffer()\n",
        "\n",
        "q, q_target = QNet(), QNet()\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "mu, mu_target = MuNet(), MuNet()\n",
        "mu_target.load_state_dict(mu.state_dict())\n",
        "\n",
        "score = 0.0\n",
        "print_interval = 20\n",
        "min_play_reward = 0\n",
        "iterations = 100000\n",
        "\n",
        "mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu)\n",
        "q_optimizer  = optim.Adam(q.parameters(), lr=lr_q)\n",
        "ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1))\n",
        "\n",
        "\n",
        "def play_game():\n",
        "    done = False\n",
        "    state = env.reset()    \n",
        "    while(not done):        \n",
        "        a = mu(torch.from_numpy(s).float()) \n",
        "        a = a.item() + ou_noise()[0]\n",
        "        s_prime, r, done, info = env.step([a])\n",
        "        env.render()\n",
        "        state = s_prime   \n",
        "\n",
        "\n",
        "for iteration in range(iterations):\n",
        "    s = env.reset()        \n",
        "    for t in range(300): # maximum length of episode is 200 for Pendulum-v0\n",
        "        a = mu(torch.from_numpy(s).float()) \n",
        "        a = a.item() + ou_noise()[0]\n",
        "        s_prime, r, done, info = env.step([a])\n",
        "        memory.put((s,a,r/100.0,s_prime,done))\n",
        "        score +=r\n",
        "        s = s_prime\n",
        "\n",
        "        if done:\n",
        "            if score/print_interval > min_play_reward:\n",
        "                play_game()\n",
        "            break              \n",
        "                \n",
        "    if memory.size()>2000:\n",
        "        for i in range(10):\n",
        "            train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer)\n",
        "            soft_update(mu, mu_target)\n",
        "            soft_update(q,  q_target)\n",
        "        \n",
        "    if iteration%print_interval==0 and iteration!=0:\n",
        "        print(\"# of episode :{}, avg score : {:.1f}\".format(iteration, score/print_interval))\n",
        "        score = 0.0\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "y5S8fFCx032Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}