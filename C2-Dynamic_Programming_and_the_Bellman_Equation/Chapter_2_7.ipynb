{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_2_7.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C2-Dynamic_Programming_and_the_Bellman_Equation/Chapter_2_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import system, name\n",
        "import time\n",
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make('FrozenLake-v0')\n",
        "env.reset()\n",
        "\n",
        "def clear():\n",
        "    if name == 'nt': \n",
        "        _ = system('cls')    \n",
        "    else: \n",
        "        _ = system('clear') \n",
        "\n",
        "def act(V, env, gamma, policy, state, v):\n",
        "    for action, action_prob in enumerate(policy[state]):                \n",
        "        for state_prob, next_state, reward, end in env.P[state][action]:                                        \n",
        "            v += action_prob * state_prob * (reward + gamma * V[next_state])                    \n",
        "            V[state] = v\n",
        "            \n",
        "def evaluate(V, action_values, env, gamma, state):\n",
        "    for action in range(env.nA):\n",
        "        for prob, next_state, reward, terminated in env.P[state][action]:\n",
        "            action_values[action] += prob * (reward + gamma * V[next_state])\n",
        "    return action_values\n",
        "\n",
        "def lookahead(env, state, V, gamma):\n",
        "    action_values = np.zeros(env.nA)\n",
        "    return evaluate(V, action_values, env, gamma, state)\n",
        "\n",
        "def improve_policy(env, gamma=1.0, terms=1e9):    \n",
        "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "    evals = 1\n",
        "    for i in range(int(terms)):\n",
        "        stable = True       \n",
        "        V = eval_policy(policy, env, gamma=gamma)\n",
        "        for state in range(env.nS):\n",
        "            current_action = np.argmax(policy[state])\n",
        "            action_value = lookahead(env, state, V, gamma)\n",
        "            best_action = np.argmax(action_value)\n",
        "            if current_action != best_action:\n",
        "                stable = False                \n",
        "                policy[state] = np.eye(env.nA)[best_action]\n",
        "            evals += 1                \n",
        "            if stable:\n",
        "                return policy, V\n",
        "\n",
        "def eval_policy(policy, env, gamma=1.0, terms=10):     \n",
        "    V = np.zeros(env.nS)    \n",
        "    for i in range(terms): \n",
        "        for state in range(env.nS):            \n",
        "            act(V, env, gamma, policy, state, v=0.0)         \n",
        "        clear()\n",
        "        print(V)\n",
        "        time.sleep(1)        \n",
        "    return V\n",
        "\n",
        "def value_iteration(env, gamma=1.0, theta=1e-9, terms=1e9):\n",
        "    V = np.zeros(env.nS)\n",
        "    for i in range(int(terms)):\n",
        "        delta = 0\n",
        "        for state in range(env.nS):\n",
        "            action_value = lookahead(env, state, V, gamma)\n",
        "            best_action_value = np.max(action_value)\n",
        "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
        "            V[state] = best_action_value             \n",
        "        if delta < theta: break\n",
        "    policy = np.zeros([env.nS, env.nA])\n",
        "    for state in range(env.nS):\n",
        "        action_value = lookahead(env, state, V, gamma)\n",
        "        best_action = np.argmax(action_value)\n",
        "        policy[state, best_action] = 1.0\n",
        "    return policy, V\n",
        "\n",
        "#policy, V = improve_policy(env.env) \n",
        "#print(policy, V)\n",
        "\n",
        "policy, V = value_iteration(env.env)\n",
        "print(policy, V)\n"
      ],
      "metadata": {
        "id": "Q5uOmbO3jE5j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}