{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Chapter_2_5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianiRiccardo90/RL-4Games/blob/master/C2-Dynamic_Programming_and_the_Bellman_Equation/Chapter_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import system, name\n",
        "import time\n",
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make('FrozenLake-v0')\n",
        "env.reset()\n",
        "\n",
        "def clear():\n",
        "    if name == 'nt': \n",
        "        _ = system('cls')    \n",
        "    else: \n",
        "        _ = system('clear') \n",
        "\n",
        "def act(V, env, gamma, policy, state, v):\n",
        "    for action, action_prob in enumerate(policy[state]):                \n",
        "        for state_prob, next_state, reward, end in env.P[state][action]:                                        \n",
        "            v += action_prob * state_prob * (reward + gamma * V[next_state])                    \n",
        "            V[state] = v\n",
        "            \n",
        "def eval_policy(policy, env, gamma=1.0, theta=1e-9, terms=1e9):     \n",
        "    V = np.zeros(env.nS)  \n",
        "    delta = 0\n",
        "    for i in range(int(terms)): \n",
        "        for state in range(env.nS):            \n",
        "            act(V, env, gamma, policy, state, v=0.0)         \n",
        "        clear()\n",
        "        print(V)\n",
        "        time.sleep(1) \n",
        "        v = np.sum(V)\n",
        "        if v - delta < theta:\n",
        "            return V\n",
        "        else:\n",
        "            delta = v\n",
        "    return V\n",
        "\n",
        "policy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\n",
        "V = eval_policy(policy, env.env) \n",
        "\n",
        "print(policy, V)\n"
      ],
      "metadata": {
        "id": "IOg65eZRi2IV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}